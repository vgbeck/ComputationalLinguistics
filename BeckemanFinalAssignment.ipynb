{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmXXVEH3SYqo1UIKRPzOcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgbeck/ComputationalLinguistics/blob/main/BeckemanFinalAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what kind of model you chose and why:\n",
        "\n",
        "I chose an RNN model, specifically LSTM. The reason that I chose this model is because it is useful for flexibility in model design. I wanted to be able to play around with my input sizes and layers to see the effects. I tried a variety of paramaters until I found my best options. It is also useful for handling complex data. RNNs and LSTMs are useful in handling sequential data such as lyrics that contain words and phrases in order so they are good for text analysis.\n",
        "\n",
        "Explain exactly how you are calculating accuracy:\n",
        "\n",
        "Accuracy is calculated by comparing the number of correctly predicted authors over the total number of predictions. For training this is done at the end of every epoch round. As I am training on 5 epochs there are five training accuracies. The average of all of these accuracies is then taken for a final score. The testing accuracy is only taken once as the model is already trained and is not split up into epochs. The testing accuracy was 7.13% and the overall training accuracy was 47.4% (this is taken from the average of all of the epochs as seen printed). As the training accuracy is much higher than the testing accuracy this could indicate overfitting.\n",
        "\n",
        "Errors this model makes:\n",
        "\n",
        "Some errors that this model makes might be due to overfitting. With this type of data it is possible that there are many occurrences of overfitting. Each of these lines has very specific song lyrics and words so the model might be trained too closely to the examples and unable to classify the testing data that it has not seen before. One of the first patterns that I saw was that out of all of the \"correct\" predictions, 80% of them were from the song genre. This tells me that my model is better at predicting song targets instead of poem targets. Looking more specifically at the incorrect prediction I see matching genres between the correct and incorrect answers. It is interesting to see how many of the incorrect predictions come from similar styles of music as their target. For example predicting Charlie Puth when the target was Mac Miller. These artists are both pop and might have similar word choices throughout their lyrics. A way to fix this might be to partition the data into training, validation, and testing sets to increase further data seperation. Having more data points would also increase the variety.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Oe9fFogDYp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #Pytorch is a Python module that can create neural networks  and automatically do backpropogation for training a network.\n",
        "import torch.nn as nn #Torch.nn is a submodule of torch that can create various types of networks and functions that operate on them.\n",
        "\n",
        "\n",
        "# single-direction RNN, optionally tied embeddings\n",
        "class Emb_RNN(nn.Module):\n",
        "    def __init__(self, params, use_LSTM=False):\n",
        "        super(Emb_RNN, self).__init__()\n",
        "        self.d_embs = params['d_emb']\n",
        "        self.d_hid =  params['d_hid']\n",
        "        self.embeddings= nn.Embedding(params['num_wds'], self.d_embs)\n",
        "        self.use_LSTM = use_LSTM\n",
        "        # input to recurrent layer, default nonlinearity is tanh\n",
        "        if use_LSTM:\n",
        "            self.i2R = nn.LSTMCell(self.d_embs, self.d_hid)\n",
        "        else:\n",
        "            self.i2R = nn.RNNCell(self.d_embs, self.d_hid)\n",
        "        # recurrent to output layer\n",
        "        self.R2o = nn.Linear(self.d_hid, params['num_authors'])\n",
        "\n",
        "    def forward(self, wd_indices):\n",
        "        for j, wd_ix in enumerate(wd_indices):\n",
        "            embs = self.embeddings(wd_ix)\n",
        "            embs = torch.unsqueeze(embs, 0)\n",
        "            if self.use_LSTM:\n",
        "                if j == 0:\n",
        "                    hidden, context = self.i2R(embs)\n",
        "                else:\n",
        "                    hidden, context = self.i2R(embs, (hidden, context))\n",
        "            else:\n",
        "                if j == 0:\n",
        "                    hidden = self.i2R(embs)\n",
        "                else:\n",
        "                    hidden = self.i2R(embs, hidden)\n",
        "        pred = self.R2o(hidden)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "yU0P40RYH4h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from model import Emb_RNN\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "verbose = False\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "#BEFORE\n",
        "d_emb = 128\n",
        "n_layers = 1\n",
        "d_hid = 128\n",
        "lr = 0.0002\n",
        "\n",
        "\n",
        "# d_emb = 128\n",
        "# n_layers = 1\n",
        "# d_hid = 128\n",
        "# lr = 0.0003\n",
        "\n",
        "use_LSTM = True\n",
        "if use_LSTM:\n",
        "    model_type = 'lstm'\n",
        "else:\n",
        "    model_type = 'rnn'"
      ],
      "metadata": {
        "id": "vCmF3z53FFQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, lines, params):\n",
        "    criterion = nn.CrossEntropyLoss() #Don't use ignore index!!!\n",
        "    optimiser = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    if os.path.exists(params['save_path']):\n",
        "        checkpoint = torch.load(params['save_path'])\n",
        "        print('Loading checkpoint')\n",
        "        net.load_state_dict(checkpoint['net_state_dict'])\n",
        "        optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "        net.eval()\n",
        "\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "        print(\"epoch \", epoch)\n",
        "        ep_loss = 0.\n",
        "        num_tested = 0\n",
        "        num_correct = 0\n",
        "        for counter, i in enumerate(torch.randperm(len(lines))):\n",
        "        #for counter, i in enumerate(torch.randperm(20)):\n",
        "            line = torch.LongTensor([wd2ix[wd] for wd in lines[i][0]])\n",
        "            #here\n",
        "\n",
        "\n",
        "            pred = net(line)\n",
        "            pred = pred.contiguous().view(-1, pred.size(-1))\n",
        "            target = torch.tensor(au2ix[lines[i][1]])\n",
        "            target = target.contiguous().view(-1)\n",
        "            target = target.long()\n",
        "            with torch.no_grad():\n",
        "                pred_numpy = np.argmax(pred.numpy(), axis=1).tolist()\n",
        "                target_numpy = target.numpy().tolist()\n",
        "                num_tested += 1\n",
        "                if pred_numpy == target_numpy:\n",
        "                    num_correct += 1\n",
        "            loss = criterion(pred, target)\n",
        "            if torch.isnan(loss):\n",
        "                with torch.no_grad():\n",
        "                    print(pred, target, lines[i])\n",
        "                    exit()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "            ep_loss += loss.detach()\n",
        "        print('Epoch', epoch, 'Accuracy', round(num_correct / num_tested, 4), 'Loss', ep_loss)\n",
        "        print('Saving checkpoint')\n",
        "        torch.save({'net_state_dict': net.state_dict(),  'optimiser_state_dict': optimiser.state_dict()}, params['save_path'])\n"
      ],
      "metadata": {
        "id": "uY5U3uGUFIqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, lines, params):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    if os.path.exists(params['save_path']):\n",
        "        checkpoint = torch.load(params['save_path'])\n",
        "        print('Loading checkpoint')\n",
        "        net.load_state_dict(checkpoint['net_state_dict'])\n",
        "        optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "        net.eval()\n",
        "    num_tested = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        current_author = None\n",
        "        scores = np.zeros(len(authors))\n",
        "        for line in lines:\n",
        "            wds = torch.LongTensor([wd2ix[wd] for wd in line[0]])\n",
        "            pred = net(wds)\n",
        "            pred = pred.contiguous().view(-1, pred.size(-1))\n",
        "            target = torch.tensor(au2ix[line[1]])\n",
        "            target = target.contiguous().view(-1)\n",
        "            target = target.long()\n",
        "            pred_numpy = np.argmax(pred.numpy(), axis=1).tolist()\n",
        "            target_numpy = target.numpy().tolist()\n",
        "            num_tested += 1\n",
        "            if pred_numpy == target_numpy:\n",
        "                num_correct += 1\n",
        "            author = au2ix[line[1]]\n",
        "            if author != current_author:\n",
        "                #We have moved to a new author so we want to collect the scores of the most recent author\n",
        "                if current_author is not None:\n",
        "                    #If we are not at the very beginning\n",
        "                    most_frequent_author = ix2au[str(np.argmax(scores))] #This author was predicted the most of all the lines of the most recent current author\n",
        "                    print(\"Current Author:\", ix2au[str(current_author)], \"Predicted Author:\", most_frequent_author) #Does the current author match the most frequently predicted author?\n",
        "                    print(scores)\n",
        "                    print(scores[current_author]) #How many hits did the current author actually get?\n",
        "                    scores = np.zeros(len(authors)) #Reset the scores to zero for each author\n",
        "                current_author = author #Reset the current author to the most recent author\n",
        "            scores[pred_numpy[0]] += 1 #Add 1 to the score of the predicted author for the last lline seen\n",
        "    print('Test accuracy', round(num_correct / num_tested, 4))\n"
      ],
      "metadata": {
        "id": "KTLaoUpBFLct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#net = Emb_RNN()\n",
        "vocab = []\n",
        "authors = []\n",
        "train_lines = []\n",
        "test_lines = []\n",
        "\n",
        "with open('trainfile-1.json', 'r') as f0:\n",
        "    train_data = json.load(f0)\n",
        "    for pair in train_data:\n",
        "        line = pair[0].split()\n",
        "        line = [re.sub(r'[^a-zA-Z\\*’\\']', '', wd.lower()) for wd in line]\n",
        "        for wd in line:\n",
        "            wd = re.sub(r'[^a-zA-Z\\*’\\']', '', wd)\n",
        "            if wd.lower() not in vocab:\n",
        "                vocab.append(wd.lower())\n",
        "        if pair[1] not in authors:\n",
        "            authors.append(pair[1])\n",
        "        train_lines.append([line, pair[1]])\n",
        "\n",
        "with open('testfile-1.json', 'r') as f0:\n",
        "    test_data = json.load(f0)\n",
        "    for pair in test_data:\n",
        "        line = pair[0].split()\n",
        "        line = [re.sub(r'[^a-zA-Z\\*’\\']', '', wd.lower()) for wd in line]\n",
        "        for wd in line:\n",
        "            wd = re.sub(r'[^a-zA-Z\\*’\\']', '', wd)\n",
        "            if wd.lower() not in vocab:\n",
        "                vocab.append(wd.lower())\n",
        "        if pair[1] not in authors:\n",
        "            authors.append(pair[1])\n",
        "        test_lines.append([line, pair[1]])\n",
        "\n",
        "\n",
        "print('There are', len(vocab), 'words in the vocabulary')\n",
        "print('There are', len(authors), 'authors')\n",
        "print(authors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02vHXP9_FRnF",
        "outputId": "e4de89d5-335e-4601-b5dd-22111c64f336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 6212 words in the vocabulary\n",
            "There are 40 authors\n",
            "['Beyonce', 'BillyJoel', 'caamp', 'CatStevens', 'Cavetown', 'CharliePuth', 'Coldplay', 'ConanGray', 'DanReynolds', 'DominicFike', 'Drake', 'GratefulDead', 'GregoryAlanIsakov', 'HarryStyles', 'JohnLennon', 'joji', 'KhalilGhibran', 'LadyGaga', 'LandonConrath', 'LucyDacus', 'MacMiller', 'MargaretAtwood', 'MaryOliver', 'MayaAngelou', 'MichaelJackson', 'NikDay', 'NoahKahan', 'NoelGallagher', 'Olivia_Rodrigo', 'Quadeca', 'Queen', 'RickAstley', 'SamSmith', 'SmashMouth', 'solange', 'StevieNicks', 'TaylorSwift', 'TheAllAmericanRejects', 'wallows', 'ZachBryan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data transferred into representation for my model\n",
        "wd2ix = {}\n",
        "ix2wd = {}\n",
        "for i,wd in enumerate(vocab):\n",
        "    wd2ix[wd] = i\n",
        "    ix2wd[str(i)] = wd\n",
        "#words_as_indices = [torch.LongTensor([wd2ix[wd] for wd in vocab])]\n",
        "\n",
        "\n",
        "au2ix = {}\n",
        "ix2au = {}\n",
        "for i,au in enumerate(authors):\n",
        "    au2ix[au] = i\n",
        "    ix2au[str(i)] = au\n",
        "#authors_as_indices = [torch.LongTensor([au2ix[au] for au in authors])]\n",
        "print(au2ix.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp0pHFvLFTs2",
        "outputId": "254eeda3-74ef-40b8-8915-d267ef8d2bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('Beyonce', 0), ('BillyJoel', 1), ('caamp', 2), ('CatStevens', 3), ('Cavetown', 4), ('CharliePuth', 5), ('Coldplay', 6), ('ConanGray', 7), ('DanReynolds', 8), ('DominicFike', 9), ('Drake', 10), ('GratefulDead', 11), ('GregoryAlanIsakov', 12), ('HarryStyles', 13), ('JohnLennon', 14), ('joji', 15), ('KhalilGhibran', 16), ('LadyGaga', 17), ('LandonConrath', 18), ('LucyDacus', 19), ('MacMiller', 20), ('MargaretAtwood', 21), ('MaryOliver', 22), ('MayaAngelou', 23), ('MichaelJackson', 24), ('NikDay', 25), ('NoahKahan', 26), ('NoelGallagher', 27), ('Olivia_Rodrigo', 28), ('Quadeca', 29), ('Queen', 30), ('RickAstley', 31), ('SamSmith', 32), ('SmashMouth', 33), ('solange', 34), ('StevieNicks', 35), ('TaylorSwift', 36), ('TheAllAmericanRejects', 37), ('wallows', 38), ('ZachBryan', 39)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di3hUjuCH1yJ",
        "outputId": "c63026e4-e7f6-4791-c288-5ad469652f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calling train\n",
            "epoch  0\n",
            "Epoch 0 Accuracy 0.1689 Loss tensor(51213.2734)\n",
            "Saving checkpoint\n",
            "epoch  1\n",
            "Epoch 1 Accuracy 0.3698 Loss tensor(39469.5859)\n",
            "Saving checkpoint\n",
            "epoch  2\n",
            "Epoch 2 Accuracy 0.5088 Loss tensor(30865.8359)\n",
            "Saving checkpoint\n",
            "epoch  3\n",
            "Epoch 3 Accuracy 0.6159 Loss tensor(24145.3301)\n",
            "Saving checkpoint\n",
            "epoch  4\n",
            "Epoch 4 Accuracy 0.7065 Loss tensor(18632.7891)\n",
            "Saving checkpoint\n",
            "calling test\n",
            "Loading checkpoint\n",
            "Current Author: Beyonce Predicted Author: DominicFike\n",
            "[ 1.  5.  1.  1.  1.  0.  2.  6.  6. 16.  2.  0.  1.  0.  0.  0.  0.  0.\n",
            "  0.  0.  1. 12.  0.  0.  2.  2.  6.  1.  0.  2.  1.  0.  0.  0.  0.  4.\n",
            "  3.  0.  3.  0.]\n",
            "1.0\n",
            "Current Author: BillyJoel Predicted Author: MargaretAtwood\n",
            "[1. 1. 1. 1. 0. 0. 2. 2. 3. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 8. 0. 3.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 2. 3. 0.]\n",
            "1.0\n",
            "Current Author: caamp Predicted Author: Beyonce\n",
            "[2. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 2. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
            "2.0\n",
            "Current Author: CatStevens Predicted Author: Beyonce\n",
            "[6. 2. 0. 2. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 2. 0. 0. 5. 2. 0. 1. 0. 0.\n",
            " 0. 0. 1. 0. 2. 2. 0. 0. 1. 0. 1. 0. 1. 1. 2. 0.]\n",
            "2.0\n",
            "Current Author: Cavetown Predicted Author: LadyGaga\n",
            "[1. 0. 0. 0. 0. 2. 1. 1. 2. 1. 0. 0. 0. 0. 0. 0. 0. 6. 0. 5. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 2. 0. 3. 0.]\n",
            "0.0\n",
            "Current Author: CharliePuth Predicted Author: MacMiller\n",
            "[1. 0. 0. 0. 1. 2. 2. 5. 0. 3. 5. 0. 3. 1. 0. 0. 0. 0. 3. 5. 6. 0. 0. 2.\n",
            " 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 5. 0.]\n",
            "2.0\n",
            "Current Author: Coldplay Predicted Author: DanReynolds\n",
            "[ 0.  0.  1.  0.  3.  1.  0.  0. 10.  0.  0.  0.  1.  0.  0.  0.  7.  0.\n",
            "  2.  0.  0.  5.  0.  0.  0.  0.  1.  0.  3.  0.  0.  2.  2.  0.  1.  3.\n",
            "  2.  0.  0.  0.]\n",
            "0.0\n",
            "Current Author: ConanGray Predicted Author: LucyDacus\n",
            "[7. 0. 0. 0. 3. 2. 1. 2. 1. 3. 0. 2. 0. 0. 0. 0. 0. 5. 1. 8. 0. 0. 0. 0.\n",
            " 0. 0. 2. 0. 2. 3. 0. 1. 3. 1. 0. 1. 3. 2. 1. 0.]\n",
            "2.0\n",
            "Current Author: DanReynolds Predicted Author: wallows\n",
            "[ 1.  2.  0.  0.  1.  0.  4.  2.  0.  0.  1.  0.  1.  0.  1.  0.  0.  0.\n",
            "  6.  0.  0.  3.  0.  7.  3.  1.  0.  7.  1.  0.  0.  1.  0.  0.  0.  0.\n",
            "  1.  0. 12.  0.]\n",
            "0.0\n",
            "Current Author: DominicFike Predicted Author: MargaretAtwood\n",
            "[ 6.  2.  0.  0.  0.  1.  0.  3.  2.  0.  2.  1.  0.  0.  0.  0.  0.  2.\n",
            " 11.  2.  1. 13.  0.  3.  1.  0.  0.  1.  0.  0.  1.  0.  6.  0.  2.  0.\n",
            "  4.  1.  0.  0.]\n",
            "0.0\n",
            "Current Author: Drake Predicted Author: JohnLennon\n",
            "[3. 2. 0. 0. 0. 0. 0. 3. 0. 3. 1. 0. 0. 1. 8. 0. 0. 1. 7. 2. 0. 1. 0. 1.\n",
            " 2. 1. 0. 2. 4. 4. 5. 2. 3. 3. 0. 0. 0. 0. 3. 0.]\n",
            "1.0\n",
            "Current Author: GratefulDead Predicted Author: ConanGray\n",
            "[1. 1. 0. 0. 0. 0. 0. 2. 1. 1. 0. 0. 1. 2. 1. 0. 0. 0. 2. 1. 0. 2. 0. 1.\n",
            " 0. 0. 0. 0. 1. 2. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "0.0\n",
            "Current Author: GregoryAlanIsakov Predicted Author: ConanGray\n",
            "[2. 4. 0. 0. 1. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 2. 0. 0.\n",
            " 0. 1. 0. 0. 0. 2. 0. 0. 0. 0. 0. 4. 1. 0. 0. 0.]\n",
            "0.0\n",
            "Current Author: HarryStyles Predicted Author: CatStevens\n",
            "[1. 0. 0. 4. 0. 1. 0. 0. 0. 4. 1. 0. 0. 0. 0. 0. 0. 3. 3. 0. 1. 2. 0. 0.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 3. 0. 2. 0.]\n",
            "0.0\n",
            "Current Author: JohnLennon Predicted Author: Queen\n",
            "[0. 2. 1. 0. 0. 1. 1. 4. 0. 3. 0. 0. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 3.\n",
            " 0. 1. 0. 0. 0. 0. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "0.0\n",
            "Current Author: joji Predicted Author: NoahKahan\n",
            "[3. 0. 0. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 0. 2. 2. 1. 0. 1. 0. 0.\n",
            " 0. 0. 6. 3. 2. 0. 2. 1. 0. 0. 0. 0. 6. 0. 1. 0.]\n",
            "2.0\n",
            "Current Author: KhalilGhibran Predicted Author: MargaretAtwood\n",
            "[ 0.  4.  0.  0.  0.  0.  0.  1.  3.  0.  0.  0.  2.  0.  0.  0.  1.  1.\n",
            "  1.  0.  0. 11.  3.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  2.  4.\n",
            "  1.  0.  0.  0.]\n",
            "1.0\n",
            "Current Author: LadyGaga Predicted Author: JohnLennon\n",
            "[ 0.  1.  0.  0.  0.  1.  2.  5.  5.  1.  1.  0.  0.  0. 21.  0.  0.  1.\n",
            "  0.  0.  2.  0.  0.  0.  0.  2.  0.  0.  0.  7.  0.  0.  1.  0.  4.  1.\n",
            "  0.  3.  0.  0.]\n",
            "1.0\n",
            "Current Author: LandonConrath Predicted Author: MargaretAtwood\n",
            "[ 0.  4.  0.  0.  0.  1.  0.  0.  6.  0.  0.  0.  2.  0.  0.  0.  2.  4.\n",
            "  8.  0.  2. 13.  0.  2.  0.  1.  2.  0.  1.  0.  0.  6.  1.  0.  0.  0.\n",
            "  0.  3.  0.  0.]\n",
            "8.0\n",
            "Current Author: LucyDacus Predicted Author: LucyDacus\n",
            "[3. 1. 0. 1. 0. 0. 0. 3. 0. 0. 1. 1. 2. 0. 0. 1. 0. 1. 1. 4. 1. 2. 2. 2.\n",
            " 1. 2. 2. 2. 1. 1. 1. 0. 1. 3. 0. 1. 1. 4. 1. 0.]\n",
            "4.0\n",
            "Current Author: MacMiller Predicted Author: MacMiller\n",
            "[ 3.  2.  0.  0.  2.  0.  0.  1.  3.  1.  3.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  2. 13.  0.  0.  0.  2.  0.  3.  0.  2.  4.  0.  0.  1.  0.  1.  0.\n",
            "  1.  1.  0.  0.]\n",
            "13.0\n",
            "Current Author: MargaretAtwood Predicted Author: MayaAngelou\n",
            "[0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 3. 3. 5.\n",
            " 3. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 2. 0.]\n",
            "3.0\n",
            "Current Author: MaryOliver Predicted Author: MargaretAtwood\n",
            "[ 1.  1.  0.  0.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  1.\n",
            "  2.  0.  0. 10.  4.  2.  1.  5.  0.  1.  1.  0.  1.  0.  0.  0.  0.  1.\n",
            "  0.  0.  0.  0.]\n",
            "4.0\n",
            "Current Author: MayaAngelou Predicted Author: MargaretAtwood\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 4. 1. 1.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.]\n",
            "1.0\n",
            "Current Author: MichaelJackson Predicted Author: Quadeca\n",
            "[ 0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  1.  5.  3.  0.  2.  2.  0.  2.\n",
            "  4.  1. 25.  1.  0.  0.  4.  2.  0.  0.  1. 34.  0.  0.  1.  0.  5.  3.\n",
            "  0.  0.  0.  0.]\n",
            "4.0\n",
            "Current Author: NikDay Predicted Author: NikDay\n",
            "[ 0.  1.  0.  0.  0.  0.  3.  0.  3.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
            "  4.  0.  0.  3.  0.  0.  0. 20.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.\n",
            "  2.  1.  0.  0.]\n",
            "20.0\n",
            "Current Author: NoahKahan Predicted Author: Olivia_Rodrigo\n",
            "[2. 0. 0. 1. 4. 0. 0. 0. 0. 0. 6. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 0.\n",
            " 1. 0. 0. 0. 8. 1. 2. 3. 2. 0. 1. 0. 2. 1. 1. 0.]\n",
            "0.0\n",
            "Current Author: NoelGallagher Predicted Author: BillyJoel\n",
            "[ 0. 10.  0.  0.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.  0.  4.  0.  0.\n",
            "  4.  0.  0.  2.  0.  2.  0.  2.  0.  0.  0.  9.  0.  0.  0.  2.  0.  2.\n",
            "  2.  0.  0.  0.]\n",
            "0.0\n",
            "Current Author: Olivia_Rodrigo Predicted Author: Olivia_Rodrigo\n",
            "[0. 2. 0. 0. 3. 1. 0. 2. 0. 0. 3. 0. 1. 0. 0. 3. 0. 1. 2. 0. 2. 2. 0. 1.\n",
            " 0. 1. 4. 0. 6. 0. 0. 0. 0. 1. 0. 0. 0. 0. 5. 0.]\n",
            "6.0\n",
            "Current Author: Quadeca Predicted Author: TaylorSwift\n",
            "[ 1.  1.  0.  1.  1.  0.  3.  6.  0.  5.  2.  0.  1.  0.  0.  0.  0. 13.\n",
            "  2.  7.  1.  2.  0.  1.  0.  0.  2.  0.  7.  3.  5.  4.  0.  0.  0. 12.\n",
            " 19.  0.  4.  0.]\n",
            "3.0\n",
            "Current Author: Queen Predicted Author: MacMiller\n",
            "[ 2.  0.  0.  0.  0.  0.  0.  2.  3.  0.  0.  0.  0.  0.  1.  0.  0.  2.\n",
            "  1.  0. 10.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.\n",
            "  5.  1.  6.  0.]\n",
            "1.0\n",
            "Current Author: RickAstley Predicted Author: RickAstley\n",
            "[ 2.  1.  0.  0.  3.  2.  2.  0.  1.  0.  6.  0.  0.  1.  6.  0.  0.  0.\n",
            "  0.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0. 25.  0.  0.  2.  0.\n",
            "  0.  0.  0.  0.]\n",
            "25.0\n",
            "Current Author: SamSmith Predicted Author: LadyGaga\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 4. 0. 1. 0. 0. 0. 0. 0. 5. 0. 2. 0. 0. 0. 0.\n",
            " 3. 1. 1. 0. 0. 0. 0. 3. 4. 0. 0. 0. 0. 0. 4. 0.]\n",
            "4.0\n",
            "Current Author: SmashMouth Predicted Author: MargaretAtwood\n",
            "[1. 0. 0. 0. 1. 6. 4. 2. 1. 0. 2. 0. 4. 0. 0. 0. 3. 1. 1. 0. 5. 7. 0. 0.\n",
            " 1. 0. 3. 0. 3. 0. 0. 2. 2. 2. 5. 3. 0. 4. 2. 0.]\n",
            "2.0\n",
            "Current Author: solange Predicted Author: StevieNicks\n",
            "[ 0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  5.\n",
            "  5.  1.  1.  0.  1.  0.  2.  0.  0.  4.  1.  4.  0.  0.  0.  0.  0. 11.\n",
            "  0.  0.  0.  0.]\n",
            "0.0\n",
            "Current Author: StevieNicks Predicted Author: Queen\n",
            "[0. 2. 0. 3. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 2. 0. 3.\n",
            " 2. 0. 0. 0. 0. 3. 6. 4. 5. 0. 0. 4. 3. 0. 6. 0.]\n",
            "4.0\n",
            "Current Author: TaylorSwift Predicted Author: LandonConrath\n",
            "[2. 1. 0. 1. 1. 0. 3. 1. 5. 1. 1. 0. 0. 0. 0. 0. 0. 0. 6. 0. 4. 3. 3. 1.\n",
            " 3. 0. 3. 0. 0. 2. 0. 0. 0. 2. 3. 1. 6. 0. 0. 0.]\n",
            "6.0\n",
            "Current Author: TheAllAmericanRejects Predicted Author: MargaretAtwood\n",
            "[3. 0. 0. 0. 2. 0. 1. 2. 0. 2. 1. 0. 3. 2. 0. 0. 0. 6. 1. 0. 0. 8. 0. 1.\n",
            " 2. 1. 0. 0. 0. 0. 6. 0. 0. 0. 0. 2. 0. 0. 5. 0.]\n",
            "0.0\n",
            "Current Author: wallows Predicted Author: ConanGray\n",
            "[2. 0. 0. 0. 2. 0. 1. 4. 0. 3. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 3. 1. 4. 0.]\n",
            "4.0\n",
            "Test accuracy 0.0713\n"
          ]
        }
      ],
      "source": [
        "params = {'num_wds': len(vocab), 'num_authors': len(authors), 'd_emb': 128, 'num_layers': 1, 'd_hid': 128, 'lr': 0.0003, 'epochs': 5, 'save_path': 'authors.pth'}\n",
        "\n",
        "model = Emb_RNN(params, True)\n",
        "\n",
        "\n",
        "for j in range(1):\n",
        "    print(\"calling train\")\n",
        "    train(model, train_lines, params)\n",
        "    print(\"calling test\")\n",
        "    test(model, test_lines, params)"
      ]
    }
  ]
}